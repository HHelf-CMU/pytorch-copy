{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HHelf-CMU/pytorch-copy/blob/main/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rLLHK5ryE1H"
      },
      "source": [
        "# Homework 1: Language Identification\n",
        "11-411/11-611 Natural Language Processing (Fall 2024)\n",
        "\n",
        "- RELEASED: Tuesday, Sep 10, 2024\n",
        "- DUE: Tuesday, Oct 1, 2024 11:59 pm EDT\n",
        "\n",
        "**Submission**: Please upload this single file named `HW1.ipynb` to [gradescope](https://www.gradescope.com/courses/557600). Ensure you do not rename the file prior to submission.\n",
        "\n",
        "\n",
        "In this assignment, you will build a language identification classifier that distinguishes between six languages:\n",
        "\n",
        "- [Hausa](https://en.wikipedia.org/wiki/Hausa_language)\n",
        "- [Indonesian](https://en.wikipedia.org/wiki/Indonesian_language)\n",
        "- [Manobo](https://en.wikipedia.org/wiki/Manobo_languages)\n",
        "- [Nahuatl](https://en.wikipedia.org/wiki/Nahuatl)\n",
        "- [Swahili](https://en.wikipedia.org/wiki/Swahili_language)\n",
        "- [Tagalog](https://en.wikipedia.org/wiki/Tagalog_language)\n",
        "\n",
        "Some languages can be distinguished easily, because they use different scripts. These six languages, however, use the same ([Latin](https://en.wikipedia.org/wiki/Latin_script)) script with minimal [diacritics](https://en.wikipedia.org/wiki/Diacritic) so it is difficult to hand-craft classifiers based on the presence or absence of particular characters. Indeed, unless you have linguistic training or familiarity with the languages, it is difficult to tell them apart.\n",
        "\n",
        "How can they be distinguished? A naïve approach is to use word counts as unigram features. However, the number of possible words in a large corpus of five languages is vast. It is essential to look at something smaller — characters.\n",
        "\n",
        "Even though the six languages use roughly the same characters, the relative frequencies of these characters vary greatly. Thus, using characters as features (unigram character models) is appealing (and fairly effective). It is also true that languages vary in their *phonotactics*, the way in which consonants and vowels combine in sequence. Thus, looking at character ngrams (for small values of $n$) is also appealing (and effective). Note, however, that as the value of $n$ increases, this approach runs into the same problem as the word unigram model (sparcity). In this scenario, the model is likely to overfit.\n",
        "\n",
        "Various kinds of classifiers can be used for this application. NB classifiers, for example, are quite effective. However, inference is slow and performance, given the same training set, is likely to be worse than other options. Simple logistic regression cannot be used because this is an n-way (multinomial) classification problem. Multinomial Logistic Regression (Softmax Regression) is a good fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLyf3G0tyE1J"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "You will perform the following tasks:\n",
        "\n",
        "1. Implement a training loop for Multinomial Logistic Regression.\n",
        "2. Implement inference for Multinomal Logistic Regression\n",
        "3. Determine the optimal order of $n$ for ngrams for MNLR trained on the training set.\n",
        "4. Calculate and display a confusion matrix for a trigram model evaluated on the test set.\n",
        "5. Inspect the feature weights, and display the most predictive features for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrCw0790yE1J"
      },
      "source": [
        "## How to Use Jupyter Notebooks for Our Assignment\n",
        "Throughout our first assignment, we'll utilize Jupyter Notebooks to walk you through concepts, let you implement them, and also allow you to experiment on your own. By the end of this assignment, you'll not only understand word embeddings more deeply but will also become familiar with a powerful tool used extensively in the data science community.\n",
        "\n",
        "### Types of Cells:\n",
        "**Markdown Cells**: These cells (like the one you're reading now) are utilized to write text, frame explanations, embed images, or even formulate equations. They make our notebook more explanatory and structured.\n",
        "\n",
        "**Code Cells**: This is where the action happens. In these cells, you'll write and execute Python code. They will play a critical role in our exercises as you experiment with word embeddings.\n",
        "\n",
        "*Warning*: Refrain from rearranging, adding, or deleting any cells.\n",
        "\n",
        "### Runtime Volatility\n",
        "As you navigate and execute the cells within this Jupyter notebook, it's crucial to understand that the runtime environment is volatile. In simpler terms:\n",
        "\n",
        "*   If you restart the notebook or experience a disconnection, all your in-memory data and variables will be lost.\n",
        "*   While the code and markdown cells will remain, the outputs from code cells will need to be regenerated by rerunning them.\n",
        "\n",
        "Therefore, if you're working on a task over an extended period or with large datasets, remember to save your results and progress frequently to avoid potential data loss.\n",
        "\n",
        "\n",
        "### Running this Notebook on Google Colab\n",
        "Google Colab is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud. To run this notebook on Colab:\n",
        "\n",
        "1.   Save a copy of this notebook on your **Google Drive**.\n",
        "2.   Open the notebook in Google Colab.\n",
        "3.   In Google Colab, you can execute each cell using the play button (or use the keyboard shortcuts mentioned above).\n",
        "\n",
        "**Tip**: Google Colab may automatically disconnect after a certain period of inactivity. Keep this in mind, especially when running longer tasks.\n",
        "\n",
        "**Note**: Google Colab provides free access to GPUs and TPUs, which might be useful for later assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZWqjeNuyE1K"
      },
      "source": [
        "## Imports\n",
        "Do not change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L2x9fT-5yE1K"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import numpy.testing as testing\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKs9Yu7nyE1K"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6KiQ1O7yE1K"
      },
      "source": [
        "### Metrics for Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PXDTvSxGyE1L"
      },
      "outputs": [],
      "source": [
        "def precision(tp: int, fp: int) -> float:\n",
        "    \"\"\"Computes the precision, given true positives and false positives.\"\"\"\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "\n",
        "def recall(tp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the recall, given the true positives and false negatives.\"\"\"\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "\n",
        "def f_measure(beta: float, tp: int, fp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the F-measure for a given beta, true positives, false positives, and false negatives.\"\"\"\n",
        "    return (\n",
        "        (1 + beta**2)\n",
        "        * (precision(tp, fp) * recall(tp, fn))\n",
        "        / (beta**2 * precision(tp, fp) * recall(tp, fn))\n",
        "    )\n",
        "\n",
        "\n",
        "def f1(tp: int, fp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the F1 measure for a given TP, FP, and FN.\"\"\"\n",
        "    return f_measure(1, tp, fp, fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM1mJJs4yE1L"
      },
      "source": [
        "### Micro-Averaged Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LxugZlTPyE1L"
      },
      "outputs": [],
      "source": [
        "def micro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged precision.\"\"\"\n",
        "    tp_sum = sum(tp.values())\n",
        "    fp_sum = sum(fp.values())\n",
        "    return tp_sum / (tp_sum + fp_sum)\n",
        "\n",
        "\n",
        "def micro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged recall.\"\"\"\n",
        "    tp_sum = sum(tp.values())\n",
        "    fn_sum = sum(fn.values())\n",
        "    return tp_sum / (tp_sum + fn_sum)\n",
        "\n",
        "\n",
        "def micro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged F1.\"\"\"\n",
        "    mp = micro_precision(tp, fp)\n",
        "    mr = micro_recall(tp, fn)\n",
        "    return 2 * (mp * mr) / (mp + mr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNG8QyAnyE1L"
      },
      "source": [
        "### Macro-Averaged Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aOdvkruIyE1L"
      },
      "outputs": [],
      "source": [
        "def macro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged precision.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (1 / n) * sum([precision(tp[c], fp[c]) for c in tp.keys()])\n",
        "\n",
        "\n",
        "def macro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged recall.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (1 / n) * sum([recall(tp[c], fn[c]) for c in tp.keys()])\n",
        "\n",
        "\n",
        "def macro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged F1.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (\n",
        "        2\n",
        "        * (macro_precision(tp, fp) * macro_recall(tp, fn))\n",
        "        / (macro_precision(tp, fp) + macro_recall(tp, fn))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60uRaPkSyE1L"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5JenndHyE1L"
      },
      "source": [
        "### Loading data from files\n",
        "\n",
        "We first need to load data from the provided TSV files. Each file is two columns, the language of the document and the document text, separated by a tab (`\\t`) character. We load this data into a list of tuples, to maintain the coupling between each document and its label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "expfX6fGyE1M"
      },
      "outputs": [],
      "source": [
        "def load_data(data_filename: str) -> list[tuple[str, str]]:\n",
        "    with open(data_filename) as fin:\n",
        "        reader = csv.reader(fin, delimiter=\"\\t\")\n",
        "        language_document_tuples = [(lang, doc) for (lang, doc) in reader]\n",
        "    return language_document_tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Y6mfWfyE1M"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "We will use ngrams as features, so we need to be able to extract them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nQYIOr2NyE1M"
      },
      "outputs": [],
      "source": [
        "def extract_ngrams(x: str, n=3) -> \"list[str]\":\n",
        "    \"\"\"Given a string, return all character ngrams of order `n`.\"\"\"\n",
        "    return [\"\".join(s) for s in (zip(*[x[i:] for i in range(n)]))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkIdpen-yE1M"
      },
      "source": [
        "### Language Codes to One-Hot Vectors\n",
        "And we need a function to convert a language code into a **one-hot vector** (called $\\mathbf{y}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RSynlhE0yE1M"
      },
      "outputs": [],
      "source": [
        "def to_onehot_vector(lang: str, langs: list[str]) -> np.ndarray:\n",
        "    y = np.zeros(len(langs))\n",
        "    y[langs.index(lang)] = 1\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZPNsUJ0yE1M"
      },
      "source": [
        "We need to be able to convert `dict`s of ngram counts to vectors of ngram counts (using a map from ngrams to dimensions of the vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4eLNWl8iyE1M"
      },
      "outputs": [],
      "source": [
        "def vectorize_ngrams(counter: dict[str, int], feature_map: dict[str, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given a dict of ngram counts and a map from features to indices, returns a vector of ngram counts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    counter : dict\n",
        "        Counter/dict of ngram counts\n",
        "    feature_map : dict\n",
        "        Map from ngrams to indices\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        A vector of ngram counts\n",
        "    \"\"\"\n",
        "    feature_vector = np.zeros(len(feature_map))\n",
        "    for ngram, count in counter.items():\n",
        "        if ngram in feature_map:\n",
        "            feature_vector[feature_map[ngram]] = count\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-A2mO8_yE1M"
      },
      "source": [
        "And putting together the conversion from text into ngrams, and vectorizing the ngrams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AbTp6E5IyE1M"
      },
      "outputs": [],
      "source": [
        "def vectorize_document(document: str, feature_map: dict[str, int], ngram_length: int) -> np.ndarray:\n",
        "    document_ngrams = extract_ngrams(document, ngram_length)\n",
        "    vector = vectorize_ngrams(Counter(document_ngrams), feature_map)\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czxhma2yE1M"
      },
      "source": [
        "We need separate functions for preprocessing the training observations and the dev/test observations. The former function must return a map from language names to labels as one-hot vectors as well as a map from features (ngrams) to indices of vector dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xJ4N-OLbyE1M"
      },
      "outputs": [],
      "source": [
        "def preprocess_training_observations(\n",
        "    training_observations: list[tuple[str, str]], n: int = 1\n",
        ") -> tuple[list[tuple[np.ndarray, np.ndarray]], dict[str, np.ndarray], dict[str, int]]:\n",
        "    langs = set()\n",
        "    features = set()\n",
        "    obs = []\n",
        "\n",
        "    for lang, doc in training_observations:\n",
        "        langs.add(lang)\n",
        "        ngrams = extract_ngrams(doc, n)\n",
        "        features = features | set(ngrams)\n",
        "        obs.append((lang, ngrams))\n",
        "    feature_map = {feature: idx for idx, feature in enumerate(sorted(features))}\n",
        "    lang_list = list(sorted(langs))\n",
        "    lang_map = {lang: to_onehot_vector(lang, lang_list) for lang in lang_list}\n",
        "\n",
        "    obs = [\n",
        "        (lang_map[lang], vectorize_ngrams(Counter(ngrams), feature_map)) for (lang, ngrams) in obs\n",
        "    ]\n",
        "\n",
        "    print(f\"{len(obs)} training observations.\")\n",
        "    return obs, lang_map, feature_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDikqEOJyE1N"
      },
      "source": [
        "The function for preprocessing test observations (and dev observations) takes the feature map and the language map as arguments and returns only the list of labeled observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SiMQaZ4gyE1N"
      },
      "outputs": [],
      "source": [
        "def preprocess_test_observations(\n",
        "    test_observations, feature_map: dict[str, int], lang_map: dict[str, np.ndarray], n: int = 1\n",
        ") -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
        "    obs = []\n",
        "\n",
        "    for i, (lang, doc) in enumerate(test_observations):\n",
        "        vectorized_doc = vectorize_document(doc, feature_map, ngram_length=n)\n",
        "        try:\n",
        "            obs.append((lang_map[lang], vectorized_doc))\n",
        "        except KeyError:\n",
        "            print(f\"Unkown language {lang} at index {i}. Known languages are: {lang_map.keys()}\")\n",
        "    print(f\"{len(obs)} test observations.\")\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsIWr3b4yE1N"
      },
      "source": [
        "### Classification Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAlUSgMTyE1N"
      },
      "source": [
        "We also need to define the softmax function.\n",
        "\n",
        "Softmax is technically\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum^K_{j=1} e^{z_j}}$$\n",
        "\n",
        "However, if implemented naïvely, this is not numerically stable. Instead, we use:\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i - \\text{max}(z)}}{\\sum^K_{j=1} e^{z_j - \\text{max}(z)}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Dz2KGujgyE1N"
      },
      "outputs": [],
      "source": [
        "def softmax(z: npt.ArrayLike) -> npt.ArrayLike:\n",
        "    \"\"\"Compute the softmax of a vector `z`\"\"\"\n",
        "    # exp(z) can get very large. For numerical stability, we subtract a vector of very large values (np.max(z)) from z.\n",
        "    return np.exp(z - np.max(z)) / np.exp(z - np.max(z)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmmpqXV2yE1N"
      },
      "source": [
        "## Training the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzWD7RGCyE1N"
      },
      "source": [
        "### Compute the gradient\n",
        "\n",
        "The formula for computing one element in our gradient is as follows (the partial derivitive of the negative log likelihood loss):\n",
        "\n",
        "$$\\frac{\\partial L_{CE}}{\\partial \\mathbf{w}_{k,i}}=-(\\mathbf{y}_k-\\hat{\\mathrm{y}}_k)\\mathbf{x}_i$$\n",
        "\n",
        "where $k$ is the **class** (rows of the matrix $\\mathbf{w}$) and $i$ corresponds the the feature (columns of the matrix $\\mathbf{w}$).\n",
        "\n",
        "We will define a function `grad` for computing the whole gradient, a $K \\times N$ matrix.\n",
        "\n",
        "**This is the first piece of code that you'll write.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "rUTieBtDyE1N",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def grad(W: np.ndarray, y: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Caculates the gradient of the negative log liklihood loss, a [K * N] matrix, with respect to W.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W : np.ndarray\n",
        "        A matrix of of weights\n",
        "    y : np.ndarray\n",
        "       The true label of the observation, expressed as a [K * N] matrix.\n",
        "    x : np.ndarray\n",
        "        A vector of features.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The gradient of the loss with respect to W.\n",
        "    \"\"\"\n",
        "    return np.outer(y-np.matmul(W, x), x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsWLPhtnyE1N"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Iterate over the observations in the training set $n$ times (in random order). For each item, compute the one-hot vector $\\mathbf{y}$ and the probability distribution $\\hat{\\mathbf{y}}$. Use these values to compute the gradient. Update the parameters based on the gradient. At the end of each epoch (pass through the training data), compute the true positives, false positives, and false negatives for each target class based on the current weights, and report micro-averaged precision and recall.\n",
        "\n",
        "**Note**: The way that we evaluated the training loop depended on an in-place shuffle of the training data, which, while not technically wrong, is maybe not the most intuitive way to do it. When shuffling your data, please either use a shuffling method that does not change the observations parameter in-place. In particular, please use `random.sample` like:\n",
        "```\n",
        "shuffled_observations = random.sample(observations, len(observations))\n",
        "```\n",
        "\n",
        "How you report the metrics is up to you — we will not look at what you output to STDOUT — but it is important that you do this. **Otherwise, you will not be able to determine whether your model is training.**\n",
        "\n",
        "You can also output the loss at each step (very noisy!), each epoch, or run the classifier on the dev set and report the metrics at the end of each epoch.\n",
        "\n",
        "Remember that the 0th column in $W$ contains the biases. You will have to insert a $1$ at the beginning of the feature vector $x$ in order to accomodate this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "qt8b3a7dyE1N",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def train(observations: tuple[np.ndarray, np.ndarray], eta: float, epochs: int = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Give a set of observations, returns a trained multinomial LR (softmax regression) model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    observations : (np.ndarray, np.ndarray)\n",
        "        Pairs consisting of a tuple of NumPy arrays (a one-hot vector encoding the ground truth language labels and a vector of features)\n",
        "    epochs : int\n",
        "        The number of epochs to train the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The moodel as a [K * N] weight matrix.\n",
        "    \"\"\"\n",
        "    W = np.multiply(np.random.rand(6, len(observations[0][1])+1), 0.01)\n",
        "    total = len(observations)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      accuracy = 0\n",
        "      confusion_matrix = np.zeros((6, 6))\n",
        "      shuffled_observations = random.sample(observations, len(observations))\n",
        "\n",
        "      for obs in shuffled_observations:\n",
        "\n",
        "        y, x = obs\n",
        "        x = np.concatenate(([1],x))\n",
        "        y_pred = np.matmul(W, x)\n",
        "        W = W + eta * grad(W, y, x)\n",
        "\n",
        "        label_true, label_pred = np.argmax(y), np.argmax(y_pred)\n",
        "        confusion_matrix[label_true, label_pred]+=1\n",
        "        if label_true == label_pred:\n",
        "          accuracy += 1\n",
        "\n",
        "      print(\"Epoch \",i,\", accuracy: \",np.round(100 * accuracy/total,2),\" %\")\n",
        "      #print(confusion_matrix)\n",
        "\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYlrRD2iyE1O"
      },
      "source": [
        "## Classification\n",
        "\n",
        "The classification function is very simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "cXDadimZyE1O",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def classify(W: np.ndarray, x: np.ndarray) -> np.intp:\n",
        "    \"\"\"\n",
        "    Return the index of the hypothesized language.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W : np.ndarray\n",
        "        Weight matrix (one row for each category/language, on column for each feature)\n",
        "    x : np.ndarray\n",
        "        Vector of real-valuled features\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    intp\n",
        "        The index of the hypothesized language.\n",
        "    \"\"\"\n",
        "    return np.argmax(np.matmul(W, np.concatenate(([1],x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvS-HsSyyE1O"
      },
      "source": [
        "## Evaluate the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVr5HQUzyE1O"
      },
      "source": [
        "Then, a function to train and evaluate the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "JfBmJWlQyE1O"
      },
      "outputs": [],
      "source": [
        "def evaluate(train_set, test_set, eta, epochs=3):\n",
        "    \"\"\"Trains and evaluates a model.\"\"\"\n",
        "    print(\"Training model\")\n",
        "    W = train(train_set, eta, epochs=epochs)\n",
        "    print(\"\\nCLASSIFY\")\n",
        "    tp, fp, fn = Counter(), Counter(), Counter()\n",
        "    for ref_lang_vec, x in test_set:\n",
        "        ref_lang = np.argmax(ref_lang_vec)\n",
        "\n",
        "        hyp_lang = classify(W, x)\n",
        "        if hyp_lang == ref_lang:\n",
        "            tp[ref_lang] += 1\n",
        "        else:\n",
        "            fp[hyp_lang] += 1\n",
        "            fn[ref_lang] += 1\n",
        "    # Print metrics\n",
        "\n",
        "    test_macro_f1 = macro_f1(tp, fp, fn)\n",
        "    test_micro_f1 = micro_f1(tp, fp, fn)\n",
        "    print(f\"macro-averaged F1:\\t\\t{test_macro_f1:.3f}\")\n",
        "    print(f\"micro-averaged F1:\\t\\t{test_micro_f1:.3f}\")\n",
        "    return test_macro_f1, test_micro_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Yi3u7fyE1O"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt6pGjk-yE1O"
      },
      "source": [
        "Load the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "F9ppF8bXyE1O"
      },
      "outputs": [],
      "source": [
        "train_observations = load_data(\"train.tsv\")\n",
        "test_observations = load_data(\"test.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zOiLs2IyE1P"
      },
      "source": [
        "### Inspecting the data\n",
        "\n",
        "Before we actually train the classifier, it's important to look at your data, and check that any assumptions you're making about it are justified. It's always useful at this point to check basic things, like:\n",
        "- How many instances of train and test data do you have?\n",
        "- What labels are in your data, and do those match between the train and test splits?\n",
        "- What is the class balance (i.e. how many instances of each class) in your dataset? Is it balanced or unbalanced?\n",
        "\n",
        "Output a dictionary for the train and test data that maps each label to the count of instances that have that label, e.g.:\n",
        "```\n",
        "{\"hausa\": 4000, \"indonesian\":...}\n",
        "```\n",
        "\n",
        "Your dictionary should be sorted in descending order of occurrence of languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGEpfqiFyE1P",
        "outputId": "45999fb3-dce0-4edb-e1d4-057eac8b540c",
        "output_for": "1.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('swahili', 377), ('tagalog', 365), ('manobo', 111), ('hausa', 91), ('nahuatl', 90), ('indonesian', 86)]\n"
          ]
        }
      ],
      "source": [
        "# this cell's output will be used for test 1.1\n",
        "dict_train = {'hausa': 0,'indonesian': 0,'manobo': 0,'nahuatl': 0,'swahili': 0,'tagalog': 0}\n",
        "for language, _ in train_observations:\n",
        "  dict_train[language] += 1\n",
        "dict_train = sorted(dict_train.items(), key=lambda x:x[1], reverse=True)\n",
        "print(dict_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ec4VSKgyE1P",
        "outputId": "d5ed19ad-5651-4062-af19-2c55435ba1fd",
        "output_for": "1.2",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('swahili', 142), ('tagalog', 121), ('nahuatl', 36), ('manobo', 34), ('indonesian', 32), ('hausa', 28)]\n"
          ]
        }
      ],
      "source": [
        "# this cell's output will be used for test 1.2\n",
        "dict_test = {'hausa': 0,'indonesian': 0,'manobo': 0,'nahuatl': 0,'swahili': 0,'tagalog': 0}\n",
        "for language, _ in test_observations:\n",
        "  dict_test[language] += 1\n",
        "dict_test = sorted(dict_test.items(), key=lambda x:x[1], reverse=True)\n",
        "print(dict_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rETq5USgyE1P"
      },
      "source": [
        "### Set hyperparameters and parameters\n",
        "\n",
        "Before training the model, we have to set the learning rate $\\eta$ and the order of the ngrams used in feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "d26VTkykyE1P"
      },
      "outputs": [],
      "source": [
        "eta = 0.0005  # Do not change this.\n",
        "epochs = 4  # Do not change this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "JmCmuEpXQ3V7"
      },
      "outputs": [],
      "source": [
        "order_of_ngrams = 4 # Change this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35um7JeyyE1P"
      },
      "source": [
        "### Running our training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7XNyGzmyE1P"
      },
      "source": [
        "Now train your classifier, and evaluate it on the test set. Vary the number of ngrams, and observe how it changes train and test F1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, lang_map, feature_map = preprocess_training_observations(\n",
        "    train_observations, n=order_of_ngrams\n",
        ")\n",
        "test_set = preprocess_test_observations(test_observations, feature_map, lang_map, n=order_of_ngrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTVdNsP7kDYd",
        "outputId": "4ef9aa5d-e1c6-4304-dd92-7ae99930f32f"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120 training observations.\n",
            "393 test observations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITzdOoUHyE1P",
        "outputId": "7011df84-eb56-445e-c6eb-72744bad65ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120 training observations.\n",
            "393 test observations.\n",
            "Training model\n",
            "Epoch  0 , accuracy:  91.7  %\n",
            "Epoch  1 , accuracy:  98.93  %\n",
            "Epoch  2 , accuracy:  96.61  %\n",
            "Epoch  3 , accuracy:  99.02  %\n",
            "\n",
            "CLASSIFY\n",
            "macro-averaged F1:\t\t0.977\n",
            "micro-averaged F1:\t\t0.982\n"
          ]
        }
      ],
      "source": [
        "train_set, lang_map, feature_map = preprocess_training_observations(\n",
        "    train_observations, n=order_of_ngrams\n",
        ")\n",
        "test_set = preprocess_test_observations(test_observations, feature_map, lang_map, n=order_of_ngrams)\n",
        "\n",
        "\n",
        "random.seed(27)\n",
        "test_macro_f1, test_micro_f1 = evaluate(train_set, test_set, eta, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4FNpx3JyE1P"
      },
      "source": [
        "Print the tuple of the best values of `(macro_f1, micro_f1)` for the model evaluated on your test set while varying the order of the ngrams. What value of n produced the best result? Why might that be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl3218WryE1P",
        "outputId": "d4c25ee1-2124-449f-9059-2344fb32bb6e",
        "output_for": "2.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.9773514178947916, 0.9821882951653944)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 2.1\n",
        "print((test_macro_f1, test_micro_f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value that produced the best results was 4-grams. My hypothesis is that 4 is the optimal value resulting of a trade-off:\n",
        "- 1 or 2-grams that represent sequences that are too short to consistently hold meaningful information\n",
        "- 5-grams or above are so large that they may capture words instead of sub-words. We then run into the sparsity issue that comes with word classification: most words are so low-frequency that they become rather uninformative."
      ],
      "metadata": {
        "id": "VthPddz5svoT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADbxCAlLyE1P"
      },
      "source": [
        "## Inspecting Classification Results\n",
        "\n",
        "We've trained our classifier, and your final F1 should be pretty close to 1.0. Great job! But what does that mean for the languages you're actually classifying? Let's rewrite our evaluation code to allow us to look at our results instance-by-instance. For this, we're going to examine the results of a re-trained trigram classifier, trained for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eQAaoAYyE1Q",
        "outputId": "b28b3ff1-4012-4e1e-bc44-d0c7bcd4b98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120 training observations.\n",
            "393 test observations.\n",
            "Epoch  0 , accuracy:  90.0  %\n",
            "macro-averaged F1:\t\t0.975\n",
            "micro-averaged F1:\t\t0.980\n"
          ]
        }
      ],
      "source": [
        "# Re-training a trigram classifier. Do not change this.\n",
        "\n",
        "INSPECTION_NGRAMS = 3\n",
        "\n",
        "train_set, lang_map, feature_map = preprocess_training_observations(\n",
        "    train_observations, n=INSPECTION_NGRAMS\n",
        ")\n",
        "test_set = preprocess_test_observations(\n",
        "    test_observations, feature_map, lang_map, n=INSPECTION_NGRAMS\n",
        ")\n",
        "\n",
        "random.seed(27)\n",
        "W_inspect = train(train_set, eta, epochs=1)\n",
        "\n",
        "## evaluate it as before. Check that this looks the same!\n",
        "tp, fp, fn = Counter(), Counter(), Counter()\n",
        "for ref_lang_vec, x in test_set:\n",
        "    ref_lang = np.argmax(ref_lang_vec)\n",
        "\n",
        "    hyp_lang = classify(W_inspect, x)\n",
        "    if hyp_lang == ref_lang:\n",
        "        tp[ref_lang] += 1\n",
        "    else:\n",
        "        fp[hyp_lang] += 1\n",
        "        fn[ref_lang] += 1\n",
        "# Print metrics\n",
        "\n",
        "print(f\"macro-averaged F1:\\t\\t{macro_f1(tp, fp, fn):.3f}\")\n",
        "print(f\"micro-averaged F1:\\t\\t{micro_f1(tp, fp, fn):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Z8m6YVyE1Q"
      },
      "source": [
        "### Writing a prediction function\n",
        "\n",
        "Write a function that takes a list of observations, and produces a list of either class indices, or class names based on a parameter. This will allow you both to look at individual results from evaluating on an existing set of data (like the test set), but also for you to evaluate your classifier on new data (i.e. any string). This will involve vectorizing the list of documents, and classifying those vectors. Once that's complete, construct an inverse language mapping, from predicted indices to the language they represent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "AQZHWlX7yE1Q",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def predict(\n",
        "    documents: list[str],\n",
        "    W: np.ndarray,\n",
        "    feature_map: dict[str, int],\n",
        "    lang_map: dict[str, np.ndarray],\n",
        "    ngram_length: int,\n",
        "    return_class_names=False,\n",
        "):\n",
        "    predict_set = []\n",
        "\n",
        "    for i, doc in enumerate(documents):\n",
        "      vectorized_doc = vectorize_document(doc, feature_map, ngram_length)\n",
        "      predict_set.append(vectorized_doc)\n",
        "\n",
        "    indices = []\n",
        "    names = []\n",
        "\n",
        "    for x in predict_set:\n",
        "        hyp_lang = classify(W, x)\n",
        "        indices.append(hyp_lang)\n",
        "\n",
        "        for lang in lang_map:\n",
        "          if np.argmax(lang_map[lang]) == hyp_lang:\n",
        "            names.append(lang)\n",
        "            break\n",
        "\n",
        "    if return_class_names:\n",
        "      return names\n",
        "    else:\n",
        "      return indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924RYujfyE1Q"
      },
      "source": [
        "Now, for each instance in the test set, print a tuple of the actual label, then the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPbGtDUayE1Q",
        "outputId": "809473d4-fe96-4667-ded8-6e49c550f9f8",
        "output_for": "3.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('swahili', 'swahili'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('hausa', 'hausa'), ('nahuatl', 'nahuatl'), ('hausa', 'hausa'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('indonesian', 'indonesian'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('hausa', 'hausa'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('indonesian', 'indonesian'), ('hausa', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('hausa', 'hausa'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('indonesian', 'indonesian'), ('manobo', 'manobo'), ('manobo', 'tagalog'), ('indonesian', 'indonesian'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('hausa', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('indonesian', 'indonesian'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('hausa', 'hausa'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('indonesian', 'indonesian'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili')]\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.1\n",
        "test_labels, test_documents = zip(*test_observations)\n",
        "test_predictions = predict(\n",
        "    test_documents,\n",
        "    W_inspect,\n",
        "    feature_map,\n",
        "    lang_map,\n",
        "    ngram_length=INSPECTION_NGRAMS,\n",
        "    return_class_names=True,\n",
        ")\n",
        "\n",
        "print(list(zip(test_labels, test_predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH5VlGV2yE1Q"
      },
      "source": [
        "### Plotting a Confusion Matrix\n",
        "\n",
        " A _confusion matrix_ is a $k \\times k$ matrix, where k is your number of classes, where the cell in position $(i,j)$ counts the number of instances that belong to class $i$ that were predicted to be in class $j$. The diagonal entries represent correct classifications; anything off of the diagonal represents an incorrect classification. The example below, from the [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html), shows a confusion matrix for a binary classification problem.\n",
        "\n",
        "![A confusion matrix for a binary classification problem](confusion_matrix.png)\n",
        "\n",
        "\n",
        "Confusion matrices can be useful to see what types of errors your classifier is making. If errors are concentrated into particular cells, it could indicate the kind of data that your classifier struggles with. Write a function to take a list of test observations, and output a numpy array that represents your classifier's confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "YAjrzsrVyE1Q"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(\n",
        "    labels: list[str], predictions: list[str], lang_map: dict[str, np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    confusion_matrix = np.zeros((len(lang_map), len(lang_map)))\n",
        "    for i in range(len(labels)):\n",
        "      label_index = np.argmax(lang_map[labels[i]])\n",
        "      prediction_index = np.argmax(lang_map[predictions[i]])\n",
        "      confusion_matrix[label_index][prediction_index] += 1\n",
        "    return confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBT2kw9UyE1Q"
      },
      "source": [
        "Now, print your confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWUEm2gLyE1Q",
        "outputId": "614dc4d5-79c4-4ca0-ad55-69867dc9fe5e",
        "output_for": "3.2",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 25.,   0.,   0.,   0.,   3.,   0.],\n",
              "       [  0.,  30.,   0.,   0.,   0.,   2.],\n",
              "       [  0.,   0.,  33.,   0.,   0.,   1.],\n",
              "       [  0.,   0.,   0.,  35.,   0.,   1.],\n",
              "       [  0.,   0.,   0.,   0., 141.,   1.],\n",
              "       [  0.,   0.,   0.,   0.,   0., 121.]])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.2\n",
        "get_confusion_matrix(test_labels, test_predictions, lang_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ3tsbqqyE1Q"
      },
      "source": [
        "From your confusion matrix, how many Hausa examples are misclassified as Swahili? Print each of the misclassified documents on a new line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQT9ECjIyE1R",
        "outputId": "da5b68d3-65d5-40c3-9a9c-6bdf9f2bce6b",
        "output_for": "3.3",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sai aka kawo kansa bisa tire, aka mika wa yarinyar, ta kuwa kai wa mahaifiyarta.\n",
            "Baku sani ba mu za mu yiwa mala'iku shari'a? Balle shari'ar al'amuran wannan rai?\n",
            "Ga wani yin ayyukan iko, ga wani kuwa annabci. Ga wani kuwa an ba shi baiwar bambance ruhohi, ga wani harsuna daban daban, kuma ga wani fassarar harsuna.\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.3\n",
        "for i in range(len(test_labels)):\n",
        "  if test_labels[i] == \"hausa\" and test_predictions[i] == \"swahili\":\n",
        "    print(test_documents[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxINVZHDyE1R"
      },
      "source": [
        "Can you formulate a hypothesis for why these Hausa examples are being misclassified as Swahili? Peruse the Wikipedia pages of the two languages, and inspect the data and features of the two languages. Consider reasons based in what you know about the languages, and about machine learning. Try to come up with 2-3 experiments you might run to validate your hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hausa - Swahili Misclassification\n",
        "\n",
        "3 examples of Hausa text are misclassified as Swahili. These two languages come from very different families: Hausa is an AfroAsiatic language spoken mostly in Nigeria, while Swahili is a Bantu language mostly spoken in Kenya and Tanzania.\n",
        "\n",
        "One possible hypothesis that could explain their misclassification is the fact that the way they are written have both been influenced by Arabic. Since the 17th century, Hausa as been written in ajami, an arabic alphabet, while Swahili was first written in arabic (and owns 15% of its vocabulary to Arabic) due to trade since the middle ages. It is possible that western translators translated Hausa and Swahili directly from Arabic scripts, meaning that the phonemes used to transcribe characters could have been the same.\n",
        "\n",
        "As a result, some very similar sequences of characters, directly resulting from this translation, could be common in both Hausa and Swahili. This would make them more difficult to distinguish for the current ML model that analyzes sequences of 3 characters, resulting in some misclassifications."
      ],
      "metadata": {
        "id": "aEDTuvlBFQ68"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LarqWkqQyE1R"
      },
      "source": [
        "### Examining Feature Weights\n",
        "\n",
        "The classifier that we've built for softmax classification behaves in many ways like using several binary softmax classifiers stacked together. The $K \\times N$ feature matrix can interpreted as a $1 \\times N$ feature vector for each class. In this section, we'll examine the weights for a few of our classified languages. To get feature names out of these vectors, we'll construct an inverted feature map, that maps indices in the feature vectors back to n-grams. Then, extract the $1 \\times N$ vector that corresponds to Hausa. Print the shape of the Hausa feature vector."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(feature_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDInldrp3Kdr",
        "outputId": "9c37844c-a5a8-4d36-d1cd-c4276ff345d4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7941"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL8Pl5iPyE1R",
        "outputId": "8c17486d-fa9d-4d00-a3b6-ce77519ba6a4",
        "output_for": "4.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7942,)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.1\n",
        "# Invert the feature map\n",
        "inverted_feature_map = {v: k for k, v in feature_map.items()}\n",
        "\n",
        "hausa_vector = W_inspect[list(lang_map.keys()).index(\"hausa\"), :]\n",
        "print(hausa_vector.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l46owXayE1R"
      },
      "source": [
        "Now, let's find the features that are most strongly predictive of an instance being Hausa. From the Hausa feature vector, find the names of the features with the top-10 positive values. Print your results with a tuple of (feature name, feature_weight) on each line:\n",
        "```\n",
        "(\"aaa\", 0.04597442104739769)\n",
        "(\"bbb\", 0.03454984682736487)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALnp3khdyE1R",
        "outputId": "e97023b0-8102-42ae-d943-53e45a22de71",
        "output_for": "4.2",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"\\t'V\", 0.0369110685638138)\n",
            "('da!', 0.02965358311839454)\n",
            "(' de', 0.026449249451550437)\n",
            "('ma!', 0.02132057379974245)\n",
            "(' si', 0.020694198989763442)\n",
            "('a c', 0.01810621347239772)\n",
            "(' te', 0.017277476867402008)\n",
            "('awd', 0.01600165911982517)\n",
            "(' a,', 0.015400966856399202)\n",
            "('wao', 0.014830338922173683)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.2\n",
        "for i in reversed(np.argsort(hausa_vector)[-10:]):\n",
        "  print(tuple((inverted_feature_map[i], hausa_vector[i])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxuk4VZryE1R"
      },
      "source": [
        "Now, for each language, print the top-10 features. Print the name of each language, and then a list of it's top-10 features on the following line, e.g.\n",
        "\n",
        "```\n",
        "hausa\n",
        "[(\"aaa\", 0.04597442104739769)...]\n",
        "indonesian\n",
        "[(\"aaa\", 0.04597442104739769)...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flCuKMqLyE1R",
        "outputId": "d27573e3-b55e-4fff-c92e-2b0e604c5369",
        "output_for": "4.3",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hausa\n",
            "[(\"\\t'V\", 0.0369110685638138), ('da!', 0.02965358311839454), (' de', 0.026449249451550437), ('ma!', 0.02132057379974245), (' si', 0.020694198989763442), ('a c', 0.01810621347239772), (' te', 0.017277476867402008), ('awd', 0.01600165911982517), (' a,', 0.015400966856399202), ('wao', 0.014830338922173683)]\n",
            "indonesian\n",
            "[(' mf', 0.019646077841738645), (' bi', 0.016718045922041472), ('bes', 0.016658417098397306), ('rao', 0.015429173213591071), ('ord', 0.014967889964555434), ('di,', 0.0135640382255355), ('ya!', 0.012768206328513306), ('a e', 0.012152608598946915), ('lib', 0.011714328253333947), ('ri!', 0.01153540341997523)]\n",
            "manobo\n",
            "[(' tr', 0.02471176063014911), ('to!', 0.020915625854751707), (' nu', 0.01902038779425626), (' oj', 0.016782030093501776), ('dio', 0.014472478582960106), ('no,', 0.014152360860422769), ('ane', 0.013504526333586762), ('nà,', 0.011873999823374882), (' ax', 0.011415190100497966), ('on,', 0.011220028645913638)]\n",
            "nahuatl\n",
            "[(' to', 0.02426531363887655), ('tle', 0.019284171748753156), ('ive', 0.016793340414958086), ('va,', 0.016295465490012127), ('leo', 0.016252911221811155), ('en,', 0.015309269186229543), ('tli', 0.014160343674942507), (' ol', 0.013147560158769512), ('kei', 0.01313472720858259), (' iw', 0.01205509774109298)]\n",
            "swahili\n",
            "[(\"\\t'V\", 0.07594948222891215), ('wa!', 0.04331996098439563), (' we', 0.03250246273667192), ('a n', 0.027224991363558833), ('ni!', 0.026615221479535166), ('atk', 0.026474440468338106), ('ikv', 0.026074649958235156), (' ho', 0.022875390645225355), ('a x', 0.0218232638808028), ('a v', 0.021648851531160165)]\n",
            "tagalog\n",
            "[(\"\\t'V\", 0.10412222490748987), ('ng!', 0.04863717704727526), (' so', 0.029404688480036204), (' ho', 0.028900546092489578), ('ald', 0.027480922138170046), (\"la'\", 0.02708286156866233), (' we', 0.024563742985423402), ('sa!', 0.0233111662350893), ('g n', 0.022744557970522227), ('ani', 0.02244741376846666)]\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.3\n",
        "for lang in lang_map.keys():\n",
        "  print(lang)\n",
        "  lang_vector = W_inspect[list(lang_map.keys()).index(lang), :]\n",
        "  top_10 = []\n",
        "  for i in reversed(np.argsort(lang_vector)[-10:]):\n",
        "    top_10.append(tuple((inverted_feature_map[i], lang_vector[i])))\n",
        "  print(top_10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJQuqbexyE1R"
      },
      "source": [
        "Choose one of the languages, and peruse its Wikipedia page or other reliable resources to learn a bit more about the language. Do the top 10 features in that language make sense given what you've learned about the structure of the language? Why or why not? Again, consider reasons stemming both from what you've learned about the language, as well as machine learning and multinomial logistic regression specifically."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nahuatl\n",
        "\n",
        "I believe that the top 10 features make sense:\n",
        "- We find multiple occurences of the sequence \"tl\"+vowel (\"tle\", \"tli\") in the top trigrams. This is highly coherent with Wikipedia's account of the /t͡ɬ/  phoneme being very common in classical nahuatl. As this sequence is atypical in most other languages, a ML classification algorithm will naturally give trigrams including \"tl\" a high weight as their chances of indicating nahuatl, and not another language, are high.\n",
        "- The most common sequence, \" to\" starts with a space. It means that a word starting with \"to\" is a good indicator of nahuatl. This also coherent: \"to-\" in nahuatl as it indicates a first-person plural possessive (https://en.wiktionary.org/wiki/to-#Classical_Nahuatl) so its frequency makes sense"
      ],
      "metadata": {
        "id": "R2IRbhm4CQcZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN8KW-cJyE1R"
      },
      "source": [
        "### Extra Credit\n",
        "\n",
        "Implement at least one of the experiments you devised to test your hypothesis regarding misclassification of Hausa as Swahili. In order to get credit you must submit not just the code and results, but also clearly describe your hypothesis, the experiment, and why the experiment is suitable to test your hypothesis. Full credit will be given to hypotheses and experiments that are well thought out, explained clearly and convincingly, and backed up with suitable evidence (computed or cited)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CjOAt7tyE1R"
      },
      "outputs": [],
      "source": [
        "# Implement extra credit here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('nlphw03')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9b5c567e963c4c8a4f65edac5dbbc9d8f011f564e4ce8a416ba289a7b299ae77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}